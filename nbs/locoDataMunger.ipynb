{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d135836",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on Thu Oct  7 12:45:55 2021\n",
    "\n",
    "@author: xusy\n",
    "\"\"\"\n",
    "\n",
    "# 25/08/2021 added gaussian smoothing function for x and y. can input gaussian window size and std\n",
    "# 26/08/2021 fixed printout of metadata currently processed in readMetaAndCount\n",
    "# 26/01/2021 added fall events\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import locoUtilities\n",
    "import datetime\n",
    "import re\n",
    "import matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863cae4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extractDateStr(s):\n",
    "    dateString = re.search(r'\\d{4}-\\d{2}-\\d{2}_\\d{2}-\\d{2}-\\d{2}', s).group()\n",
    "    dateTime = datetime.datetime.strptime(dateString, '%Y-%m-%d_%H-%M-%S')\n",
    "    return dateString, dateTime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7427fc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def readMetaAndCount(dataFolder, companionEspObj,  startMin, endMin, initialResamplePeriod, smoothing, longForm=False):\n",
    "    filelist = os.listdir(dataFolder)\n",
    "    countLogList = [s for s in filelist if \"CountLog\" in s]\n",
    "    if countLogList:\n",
    "        countLogList = np.sort(countLogList)\n",
    "        print('countLog files found: \\n')\n",
    "        print(countLogList)\n",
    "    else:\n",
    "        print('Warning: no countLog files')\n",
    "        exit()\n",
    "    metaDataList = [s for s in filelist if \"MetaData\" in s]\n",
    "    if metaDataList:\n",
    "        metaDataList = np.sort(metaDataList)\n",
    "        print('\\nmetaData files found: \\n')\n",
    "        print(metaDataList)\n",
    "    else:\n",
    "        print('Warning: no metaData files')\n",
    "        exit()\n",
    "    portLocationsList = [s for s in filelist if \"PortLocations\" in s]\n",
    "    if portLocationsList:\n",
    "        portLocationsList = np.sort(portLocationsList)\n",
    "        print('portLocations files found: \\n')\n",
    "        print(portLocationsList)\n",
    "    else:\n",
    "        print('Warning: no portlocations files')\n",
    "        exit()\n",
    "\n",
    "    feedLogList = [s for s in filelist if \"FeedLog\" in s]\n",
    "    if feedLogList:\n",
    "        feedLogList = np.sort(feedLogList)\n",
    "        print('\\nfeedLog files found: \\n')\n",
    "        print(feedLogList)\n",
    "    else:\n",
    "        if companionEspObj:\n",
    "            feedLogList = np.unique([extractDateStr(i)[0]\n",
    "                                    for i in companionEspObj.flies.ChamberID])\n",
    "            print(feedLogList)\n",
    "\n",
    "        else:\n",
    "            print('Warning: no feedlog files')\n",
    "\n",
    "    experimentSummary = []\n",
    "    for i in range(len(countLogList)):\n",
    "        companionMetaData = metaDataList[np.argmin([np.abs(extractDateStr(\n",
    "            m)[1] - extractDateStr(countLogList[i])[1]) for m in metaDataList])]\n",
    "        companionPortLocations = portLocationsList[np.argmin([np.abs(extractDateStr(\n",
    "            m)[1] - extractDateStr(countLogList[i])[1]) for m in portLocationsList])]\n",
    "        companionFeedLog = [m for m in feedLogList if np.abs(extractDateStr(\n",
    "            m)[1] - extractDateStr(countLogList[i])[1]).total_seconds() < 15]\n",
    "        if companionFeedLog:\n",
    "            companionFeedLog = companionFeedLog[0]\n",
    "            companionFeedLogDate = extractDateStr(companionFeedLog)[0]\n",
    "        else:\n",
    "            companionFeedLog = 'N/A'\n",
    "            companionFeedLogDate = 'N/A'\n",
    "        experimentSummary.append({'countLogFile': countLogList[i],\n",
    "                                  'countLogDate': extractDateStr(countLogList[i])[0],\n",
    "                                  'metaDataFile': companionMetaData,\n",
    "                                  'metaDataDate': extractDateStr(companionMetaData)[0],\n",
    "                                  'portLocationsFile': companionPortLocations,                                  'countLogDate': extractDateStr(countLogList[i])[0],\n",
    "                                  'portLocationsDate': extractDateStr(companionPortLocations)[0],\n",
    "                                  'feedLogFile': companionFeedLog,\n",
    "                                  'feedLogDate': companionFeedLogDate\n",
    "                                  })\n",
    "\n",
    "    experimentSummary = pd.DataFrame(experimentSummary)\n",
    "\n",
    "# definition:\n",
    "#   LeftPortX = the centerline  of the left capillary\n",
    "#   LeftPortY = the bottom of the left feed port\n",
    "#   etc\n",
    "\n",
    "    bigPortLocationsDf = pd.DataFrame()\n",
    "    for dataSetNumber in range(0, len(portLocationsList)):\n",
    "        portLocationsDf = pd.read_csv(\n",
    "            dataFolder + portLocationsList[dataSetNumber])\n",
    "        portLocationsDf['Date'] = portLocationsList[dataSetNumber][14:33]\n",
    "        portLocationsDf['DateChamberID'] = portLocationsDf['Date'] + \\\n",
    "            '_Chamber' + (portLocationsDf.index+1).astype(str)\n",
    "        xconv = portLocationsDf.XmmPerPix[0]\n",
    "        yconv = portLocationsDf.YmmPerPix[0]\n",
    "        portLocationsDf['ChamberTopConv'] = (\n",
    "            portLocationsDf.ChamberBottom - portLocationsDf.ChamberTop) * yconv\n",
    "        portLocationsDf['ChamberBottomConv'] = (\n",
    "            portLocationsDf.ChamberBottom - portLocationsDf.ChamberBottom) * yconv\n",
    "        portLocationsDf['ChamberLeftConv'] = (\n",
    "            portLocationsDf.ChamberLeft - portLocationsDf.ChamberLeft) * xconv\n",
    "        portLocationsDf['ChamberRightConv'] = (\n",
    "            portLocationsDf.ChamberRight - portLocationsDf.ChamberLeft) * xconv\n",
    "        portLocationsDf['PortsMidpointXConv'] = (\n",
    "            portLocationsDf.PortsMidpointX - portLocationsDf.ChamberLeft) * xconv\n",
    "        portLocationsDf['LeftPortXConv'] = (\n",
    "            portLocationsDf.LeftPortX - portLocationsDf.ChamberLeft) * xconv\n",
    "        portLocationsDf['LeftPortYConv'] = (\n",
    "            portLocationsDf.ChamberBottom - portLocationsDf.LeftPortY) * yconv\n",
    "        portLocationsDf['RightPortXConv'] = (\n",
    "            portLocationsDf.RightPortX - portLocationsDf.ChamberLeft) * xconv\n",
    "        portLocationsDf['RightPortYConv'] = (\n",
    "            portLocationsDf.ChamberBottom - portLocationsDf.RightPortY) * yconv\n",
    "        meanLeftPort = [np.mean(portLocationsDf['LeftPortXConv']), np.mean(\n",
    "            portLocationsDf['LeftPortYConv'])]\n",
    "        portLocationsDf['LeftPortXConvDev'] = portLocationsDf['LeftPortXConv'] - meanLeftPort[0]\n",
    "        portLocationsDf['LeftPortYConvDev'] = portLocationsDf['LeftPortYConv'] - meanLeftPort[1]\n",
    "        bigPortLocationsDf = pd.concat(\n",
    "            [bigPortLocationsDf, portLocationsDf], axis=0)\n",
    "\n",
    "    bigCountLogDf = pd.DataFrame()\n",
    "    bigMetaDataDf = pd.DataFrame()\n",
    "    for dataSetNumber in range(0, len(countLogList)):\n",
    "        print(countLogList[dataSetNumber])\n",
    "        experimentEntry = experimentSummary.loc[experimentSummary['countLogFile']\n",
    "                                                == countLogList[dataSetNumber]]\n",
    "        companionMetaData = experimentEntry['metaDataFile'].iloc[0]\n",
    "        print(companionMetaData)\n",
    "        companionPortLocationsDf = bigPortLocationsDf.loc[bigPortLocationsDf.Date == extractDateStr(\n",
    "            experimentEntry['portLocationsFile'].iloc[0])[0]]\n",
    "        metaDataDf = pd.read_csv(dataFolder + companionMetaData)\n",
    "        reader = pd.read_csv(\n",
    "            dataFolder + countLogList[dataSetNumber], chunksize=(endMin+1) * 60 * 30)\n",
    "        countLogDfUnselected = reader.get_chunk()\n",
    "        expectedIDs = {int(re.search(r'Ch(.*)_Obj1_X', s).group(1))\n",
    "                       for s in countLogDfUnselected.filter(regex='Obj1_X').columns}\n",
    "        existingIDs = set(metaDataDf.ID)\n",
    "        diffID = expectedIDs - existingIDs\n",
    "        if len(diffID) > 0:\n",
    "            print('MetaData is missing IDs ' + str(np.sort(list(diffID))))\n",
    "        for id in diffID:\n",
    "            todrop = countLogDfUnselected.filter(regex='Ch'+str(id)).columns\n",
    "            countLogDfUnselected = countLogDfUnselected.drop(\n",
    "                todrop.tolist(), axis=1)\n",
    "        companionPortLocationsDf = companionPortLocationsDf.loc[companionPortLocationsDf.index.isin(\n",
    "            metaDataDf.index)]\n",
    "        countLogDfTrimmed = calculateSpeedinCountLog(\n",
    "            countLogDfUnselected, companionPortLocationsDf, smoothing)\n",
    "        countLogDfTimeBanded = countLogDfTrimmed.loc[(\n",
    "            countLogDfTrimmed.Seconds > startMin * 60) & (countLogDfTrimmed.Seconds < endMin * 60)]\n",
    "        metaDataDf.columns = metaDataDf.columns.str.replace(' ', '')\n",
    "        metaDataDf['Date'] = extractDateStr(countLogList[dataSetNumber])[0]\n",
    "        countLogDfNew, countLogDfOld = locoUtilities.resampleCountLog(\n",
    "            countLogDfTimeBanded, countLogList[dataSetNumber], initialResamplePeriod, longForm)\n",
    "        countLogDfNew = correctInPortData(countLogDfNew)\n",
    "\n",
    "        if longForm is False:\n",
    "            countLogDfNew.columns = countLogList[dataSetNumber][9:28] + \\\n",
    "                '_' + countLogDfNew.columns\n",
    "        if dataSetNumber == 0:\n",
    "            bigCountLogDf = countLogDfNew\n",
    "            bigMetaDataDf = metaDataDf\n",
    "        else:\n",
    "            if longForm:\n",
    "                bigCountLogDf = pd.concat(\n",
    "                    [bigCountLogDf, countLogDfNew], axis=0)\n",
    "            else:\n",
    "                bigCountLogDf = pd.concat(\n",
    "                    [bigCountLogDf, countLogDfNew], axis=1)\n",
    "                bigMetaDataDf = pd.concat([bigMetaDataDf, metaDataDf], axis=0)\n",
    "\n",
    "    bigMetaDataDf = bigMetaDataDf.reset_index(drop=True)\n",
    "    bigMetaDataDf['Genotype'] = bigMetaDataDf['Genotype'].str.lower()\n",
    "    bigMetaDataDf['Food1'] = bigMetaDataDf['Food1'].astype(str)\n",
    "    bigMetaDataDf['Food2'] = bigMetaDataDf['Food2'].astype(str)\n",
    "    bigMetaDataDf['Starvedhrs'] = bigMetaDataDf['Starvedhrs'].astype(str)\n",
    "    bigMetaDataDf = assignStatus(bigMetaDataDf)\n",
    "    return bigMetaDataDf, bigCountLogDf, bigPortLocationsDf, experimentSummary\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8050437f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def calculateSpeedinCountLog(countLogDf, companionPortLocationsDf, smoothing, speedThreshold=30, gaussianWindowSize=10, gaussianSTD=3):\n",
    "    xconv = companionPortLocationsDf.XmmPerPix[0]\n",
    "    yconv = companionPortLocationsDf.YmmPerPix[0]\n",
    "    cx = countLogDf.filter(regex='_X') * xconv\n",
    "    cy = countLogDf.filter(regex='_Y') * yconv\n",
    "    cv = countLogDf.filter(regex='_Vpix/s')\n",
    "    ct = countLogDf.filter(regex='Seconds')\n",
    "    X = cx.rename(columns=lambda x: ''.join(\n",
    "        [str(x).split('_')[0], '_', str(x).split('_')[1]]))\n",
    "    Y = cy.rename(columns=lambda x: ''.join(\n",
    "        [str(x).split('_')[0], '_', str(x).split('_')[1]]))\n",
    "    ctc = [companionPortLocationsDf.ChamberTopConv.values for i in range(\n",
    "        0, Y.shape[0])]\n",
    "    chamberTops = pd.DataFrame(ctc, columns=Y.columns)\n",
    "    Y = chamberTops - Y\n",
    "    if smoothing:\n",
    "        # smoothing X and Y\n",
    "        XX = X.rolling(gaussianWindowSize, win_type='gaussian').mean(\n",
    "            std=gaussianSTD)\n",
    "        YY = Y.rolling(gaussianWindowSize, win_type='gaussian').mean(\n",
    "            std=gaussianSTD)\n",
    "    else:\n",
    "        XX = X\n",
    "        YY = Y\n",
    "    deltaXX = np.diff(XX, axis=0)\n",
    "    deltaYY = np.diff(YY, axis=0)\n",
    "    deltaT = np.diff(ct, axis=0)\n",
    "    VV = (deltaXX**2+deltaYY**2)**0.5/deltaT\n",
    "    VV = pd.DataFrame(np.concatenate(\n",
    "        [np.zeros([1, VV.shape[1]]), VV]), columns=cv.columns)\n",
    "    VV = VV.rename(columns=lambda x: ''.join(\n",
    "        [str(x).split('_')[0], '_', str(x).split('_')[1]]))\n",
    "    for column in VV.columns:\n",
    "        VV[column] = intrapolateUnderThreshold(\n",
    "            VV.loc[:, column], speedThreshold)\n",
    "    Vy = deltaYY/deltaT\n",
    "    Vy = pd.DataFrame(np.concatenate(\n",
    "        [np.zeros([1, Vy.shape[1]]), Vy]), columns=cv.columns)\n",
    "    Vy = Vy.rename(columns=lambda x: ''.join(\n",
    "        [str(x).split('_')[0], '_', str(x).split('_')[1]]))\n",
    "    Vx = deltaXX/deltaT\n",
    "    Vx = pd.DataFrame(np.concatenate(\n",
    "        [np.zeros([1, Vx.shape[1]]), Vx]), columns=cv.columns)\n",
    "    Vx = Vx.rename(columns=lambda x: ''.join(\n",
    "        [str(x).split('_')[0], '_', str(x).split('_')[1]]))\n",
    "\n",
    "    lp = [companionPortLocationsDf.LeftPortYConv.values for i in range(\n",
    "        0, YY.shape[0])]\n",
    "    leftPort = pd.DataFrame(lp, columns=YY.columns)\n",
    "    rp = [companionPortLocationsDf.RightPortYConv.values for i in range(\n",
    "        0, YY.shape[0])]\n",
    "    rightPort = pd.DataFrame(rp, columns=YY.columns)\n",
    "    mpX = [companionPortLocationsDf.PortsMidpointXConv.values for i in range(\n",
    "        0, YY.shape[0])]\n",
    "    midpointX = pd.DataFrame(mpX, columns=YY.columns)\n",
    "\n",
    "    InLeftPort = (1*(YY > leftPort) + 1*(XX < midpointX)) == 2\n",
    "    InRightPort = (1*(YY > rightPort) + 1*(XX > midpointX)) == 2\n",
    "\n",
    "    XX = XX.rename(columns=lambda x: str(x)+'_X')\n",
    "    YY = YY.rename(columns=lambda x: str(x)+'_Y')\n",
    "    VV = VV.rename(columns=lambda x: str(x)+'_V')\n",
    "    Vy = Vy.rename(columns=lambda x: str(x)+'_vY')\n",
    "    Vx = Vx.rename(columns=lambda x: str(x)+'_vX')\n",
    "    InLeftPort = InLeftPort.rename(columns=lambda x: str(x)+'_InLeftPort')\n",
    "    InRightPort = InRightPort.rename(columns=lambda x: str(x)+'_InRightPort')\n",
    "    newCountLog = pd.concat([countLogDf.iloc[:, [0, 1, 2]],\n",
    "                            XX, YY, VV, Vy, Vx, InLeftPort, InRightPort], axis=1)\n",
    "    return newCountLog\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca7dda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculatePeriFeedLoco(countLogDf, companionPortLocationsDf, companionEspObj, exptSum, monitorWindow=120, startSeconds=0):\n",
    "\n",
    "    feedsRevisedDf = companionEspObj.feeds\n",
    "    feedsRevisedDf['startMonitorIdx'] = np.nan\n",
    "    feedsRevisedDf['startFeedIdx'] = np.nan\n",
    "    feedsRevisedDf['startFeedIdxRevised'] = np.nan\n",
    "    feedsRevisedDf['endFeedIdx'] = np.nan\n",
    "    feedsRevisedDf['endFeedIdxRevised'] = np.nan\n",
    "    feedsRevisedDf['endMonitorIdx'] = np.nan\n",
    "    feedsRevisedDf[str(monitorWindow)+'beforeFeedSpeed_mm/s'] = np.nan\n",
    "    feedsRevisedDf['duringFeedSpeed_mm/s'] = np.nan\n",
    "    feedsRevisedDf[str(monitorWindow)+'afterFeedSpeed_mm/s'] = np.nan\n",
    "    feedsRevisedDf['revisedFeedDuration_s'] = np.nan\n",
    "    feedsRevisedDf['countLogID'] = np.nan\n",
    "\n",
    "    feedsRevisedDf = feedsRevisedDf.drop(labels=feedsRevisedDf.loc[np.isnan(\n",
    "        feedsRevisedDf['FeedDuration_s'])].index, axis=0)\n",
    "\n",
    "    # setup toolbar\n",
    "\n",
    "    print('recalculating feed duration for feeds...')\n",
    "    locoUtilities.startProgressbar()\n",
    "    for i in feedsRevisedDf.index:\n",
    "        feed = feedsRevisedDf.loc[i]\n",
    "        if feed.RelativeTime_s > startSeconds:\n",
    "            chamberID = feed['ChamberID']\n",
    "            feedDate = extractDateStr(chamberID)[0]\n",
    "            # print(feedDate)\n",
    "            # print(exptSum.loc[exptSum['feedLogDate']==feedDate])\n",
    "            countDate = exptSum.loc[exptSum['feedLogDate']\n",
    "                                    == feedDate]['countLogDate'].iloc[0]\n",
    "            countLogObjID = countDate + '_Ch' + \\\n",
    "                chamberID.split('amber')[1]+'_Obj1'\n",
    "            y = countLogDf[countLogObjID+'_Y']\n",
    "            t = countLogDf[countDate+'_Seconds']\n",
    "            v = countLogDf[countLogObjID+'_V']\n",
    "            ploc = companionPortLocationsDf.loc[companionPortLocationsDf.DateChamberID == chamberID]\n",
    "            startFeedTime = feed['RelativeTime_s']\n",
    "            endFeedTime = feed['RelativeTime_s'] + feed['FeedDuration_s']\n",
    "            startFeedIdx = np.abs(t - startFeedTime).idxmin()\n",
    "            endFeedIdx = np.abs(t - endFeedTime).idxmin()\n",
    "            outPort = 1*(y[startFeedIdx:endFeedIdx].values -\n",
    "                         ploc.LeftPortYConv.values < 0)\n",
    "            longestFeedStretch = np.cumsum(\n",
    "                outPort) == np.bincount(outPort.cumsum()).argmax()\n",
    "            longestFeedStretchIdx = [\n",
    "                j for j, l in enumerate(longestFeedStretch) if l]\n",
    "            startFeedIdx1 = t.index[t.index.get_loc(\n",
    "                startFeedIdx)+longestFeedStretchIdx[0]]\n",
    "            endFeedIdx1 = t.index[t.index.get_loc(\n",
    "                startFeedIdx)+longestFeedStretchIdx[-1]]\n",
    "            startFeedTime1 = t[startFeedIdx1]\n",
    "            endFeedTime1 = t[endFeedIdx1]\n",
    "            startMonitorTime = np.max([0, startFeedTime1 - monitorWindow])\n",
    "            startMonitorIdx = np.abs(t - startMonitorTime).idxmin()\n",
    "            endMonitorTime = np.nanmin(\n",
    "                [np.nanmax(t.values), endFeedTime1 + monitorWindow])\n",
    "            endMonitorIdx = np.abs(t - endMonitorTime).idxmin()\n",
    "            feedsRevisedDf.loc[i, 'countLogID'] = countLogObjID\n",
    "            feedsRevisedDf.loc[i, 'startMonitorIdx'] = startMonitorIdx\n",
    "            feedsRevisedDf.loc[i, 'startFeedIdx'] = startFeedIdx\n",
    "            feedsRevisedDf.loc[i, 'startFeedIdxRevised'] = startFeedIdx1\n",
    "            feedsRevisedDf.loc[i, 'endFeedIdx'] = endFeedIdx\n",
    "            feedsRevisedDf.loc[i, 'endFeedIdxRevised'] = endFeedIdx1\n",
    "            feedsRevisedDf.loc[i, 'endMonitorIdx'] = endMonitorIdx\n",
    "            feedsRevisedDf.loc[i,\n",
    "                               'revisedFeedDuration_s'] = endFeedTime1 - startFeedTime1\n",
    "            vb = np.nanmean(v[startMonitorIdx:startFeedIdx1])\n",
    "            vd = np.nanmean(v[startFeedIdx1:endFeedIdx1])\n",
    "            va = np.nanmean(v[endFeedIdx1:endMonitorIdx])\n",
    "            feedsRevisedDf.loc[i, str(monitorWindow) +\n",
    "                               'beforeFeedSpeed_mm/s'] = vb\n",
    "            feedsRevisedDf.loc[i, 'duringFeedSpeed_mm/s'] = vd\n",
    "            feedsRevisedDf.loc[i, str(monitorWindow) +\n",
    "                               'afterFeedSpeed_mm/s'] = va\n",
    "            feedsRevisedDf.loc[i, str(\n",
    "                monitorWindow)+'duringPercSpeedGain'] = ((vd - vb)/(vb))*100\n",
    "            feedsRevisedDf.loc[i, str(monitorWindow) +\n",
    "                               'afterPercSpeedGain'] = ((va - vb)/vb)*100\n",
    "            locoUtilities.drawProgressbar()\n",
    "    locoUtilities.endProgressbar()\n",
    "\n",
    "    feedsRevisedDf['revisedFeedDuration_min'] = feedsRevisedDf['revisedFeedDuration_s']/60\n",
    "    grouped_df = feedsRevisedDf.groupby(['ChamberID', 'countLogID'])\n",
    "    mean_df = grouped_df.mean()\n",
    "    mean_df.reset_index(inplace=True)\n",
    "    total_df = grouped_df.sum()\n",
    "    total_df.reset_index(inplace=True)\n",
    "    feedResults = pd.merge(mean_df, total_df, how='outer',\n",
    "                           on='ChamberID', suffixes=(\"_Mean\", \"_Total\"))\n",
    "    feedResults = feedResults.drop(columns=['countLogID_Total', 'FeedSpeed_nl/s_Total', 'RelativeTime_s_Total',  'Starved hrs_Total', 'AverageFeedSpeedPerFly_µl/s_Total', str(\n",
    "        monitorWindow)+'beforeFeedSpeed_mm/s_Total', 'duringFeedSpeed_mm/s_Total', str(monitorWindow)+'afterFeedSpeed_mm/s_Total'])\n",
    "    feedResults['duringBeforeSpeedRatio'] = feedResults['duringFeedSpeed_mm/s_Mean'] / \\\n",
    "        feedResults[str(monitorWindow)+'beforeFeedSpeed_mm/s_Mean']\n",
    "    feedResults['afterBeforeSpeedRatio'] = feedResults[str(\n",
    "        monitorWindow)+'afterFeedSpeed_mm/s_Mean'] / feedResults[str(monitorWindow)+'beforeFeedSpeed_mm/s_Mean']\n",
    "    feedVolColumns = [s.replace('_X', '_feedVol_nl')\n",
    "                      for s in countLogDf.filter(regex='_X').columns]\n",
    "    feedCountColumns = [s.replace('_X', '_feedCount')\n",
    "                        for s in countLogDf.filter(regex='_X').columns]\n",
    "    feedDurationColumns = [s.replace(\n",
    "        '_X', '_feedRevisedDuration_s') for s in countLogDf.filter(regex='_X').columns]\n",
    "    cumVolColumns = [s.replace('_X', '_cumVol')\n",
    "                     for s in countLogDf.filter(regex='_X').columns]\n",
    "    countLogDfNew = countLogDf\n",
    "    countLogDfNew.drop(list(countLogDfNew.filter(\n",
    "        regex='_feedVol_nl')), axis=1, inplace=True)\n",
    "    countLogDfNew.drop(list(countLogDfNew.filter(\n",
    "        regex='_feedCount')), axis=1, inplace=True)\n",
    "    countLogDfNew.drop(list(countLogDfNew.filter(\n",
    "        regex='_feedRevisedDuration_s')), axis=1, inplace=True)\n",
    "    countLogDfNew.drop(list(countLogDfNew.filter(\n",
    "        regex='_cumVol')), axis=1, inplace=True)\n",
    "    countLogDfNew = pd.concat([countLogDf, pd.DataFrame(\n",
    "        0, index=countLogDf.index, columns=feedVolColumns + feedCountColumns + feedDurationColumns)], axis=1)\n",
    "    print('putting feeds back into countlog...')\n",
    "    locoUtilities.startProgressbar()\n",
    "    for i in feedsRevisedDf.index:\n",
    "        countLogID = feedsRevisedDf.loc[i, 'countLogID']\n",
    "        endFeedIdxRevised = feedsRevisedDf.loc[i, 'endFeedIdxRevised']\n",
    "        countLogDfNew.loc[endFeedIdxRevised, countLogID +\n",
    "                          '_feedVol_nl'] = feedsRevisedDf.loc[i, 'FeedVol_nl']\n",
    "        countLogDfNew.loc[endFeedIdxRevised, countLogID+'_feedCount'] = 1\n",
    "        countLogDfNew.loc[endFeedIdxRevised, countLogID +\n",
    "                          '_feedRevisedDuration_s'] = feedsRevisedDf.loc[i, 'revisedFeedDuration_s']\n",
    "        # print(countLogDfNew.loc[endFeedIdxRevised])\n",
    "        # print(feedsRevisedDf.loc[i, 'revisedFeedDuration_s'])\n",
    "        # print(countLogDfNew.loc[endFeedIdxRevised, countLogID+'_feedRevisedDuration'])\n",
    "    # plt.plot(countLogDfNew[countLogDfNew.filter('_feedRevisedDuration').columns].fillna(0), 'o')\n",
    "        locoUtilities.drawProgressbar()\n",
    "    locoUtilities.endProgressbar()\n",
    "\n",
    "    durCols = countLogDfNew.filter('_feedRevisedDuration').columns\n",
    "    countLogDfNew[durCols] = countLogDfNew[durCols].fillna(0)\n",
    "    cumFeedVol = countLogDfNew.filter(regex='_feedVol').cumsum()\n",
    "    # print(cumFeedVol.columns)\n",
    "    # print(cumVolColumns)\n",
    "    cumFeedVol.columns = cumVolColumns\n",
    "    countLogDfNew = pd.concat([countLogDfNew, cumFeedVol], axis=1)\n",
    "    maxSpeed = np.ceil(np.nanmax(feedResults[[str(monitorWindow)+'beforeFeedSpeed_mm/s_Mean',\n",
    "                                              'duringFeedSpeed_mm/s_Mean', str(monitorWindow)+'afterFeedSpeed_mm/s_Mean']]))\n",
    "\n",
    "    return feedsRevisedDf, countLogDfNew, feedResults, maxSpeed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c41e517c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def labelStretches(vector):\n",
    "    vectorCopy = vector\n",
    "    invVector = 1 - vector\n",
    "    IVcumsum = invVector.cumsum()\n",
    "    IVbin = np.bincount(IVcumsum)\n",
    "    IVbinS = IVbin[IVbin > 1]\n",
    "    IVbinSU = np.unique(IVbinS)\n",
    "    idxMat = pd.DataFrame(columns=['sIdx', 'inc'])\n",
    "    n = 0\n",
    "    import time\n",
    "        \n",
    "\n",
    "    for j in range(0, len(IVbinSU)):\n",
    "        startIdx = [i for i, ivb in enumerate(IVbin == IVbinSU[j]) if ivb]\n",
    "        t1 = time.time()\n",
    "        print('start')\n",
    "     \n",
    "        for k in startIdx:\n",
    "            idxMat.loc[n, ['sIdx', 'inc']] = [k, IVbinSU[j]]\n",
    "            n = n+1\n",
    "    idxMat = idxMat.astype(int)\n",
    "    idxMat = idxMat.sort_values(by='sIdx').reset_index(drop=True)\n",
    "    t2 = time.time()\n",
    "    print(t2-t1)\n",
    "    print(idxMat)\n",
    "    if len(idxMat) > 0:\n",
    "        idxMat['inc'][1::] = idxMat['inc'][1::]-1\n",
    "        idxMat['cumInc'] = idxMat['inc'].cumsum()\n",
    "        curr = [idxMat.loc[i]['sIdx']+idxMat.loc[i-1]['cumInc']\n",
    "                for i in idxMat.index[1::]]\n",
    "        curr.append(idxMat.loc[0, 'sIdx'])\n",
    "        curr.sort()\n",
    "        idxMat['currIdx'] = curr\n",
    "\n",
    "        for i in idxMat.index:\n",
    "            # print(i)\n",
    "            vectorCopy[idxMat.loc[i, 'currIdx']:idxMat.loc[i, 'currIdx']+idxMat.loc[i, 'inc']] = i+1\n",
    "    else:\n",
    "        vectorCopy = vector\n",
    "    t3 = time.time()\n",
    "    print(t3-t2)\n",
    "\n",
    "    return vectorCopy, idxMat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d2a513b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def correctInPortData(countLogDf):\n",
    "    for column in countLogDf.filter(regex='InLeftPort').columns:\n",
    "        column\n",
    "        x = countLogDf[column] > 0\n",
    "        v, m = labelStretches(x)\n",
    "        countLogDf[column] = v\n",
    "    for column in countLogDf.filter(regex='InRightPort').columns:\n",
    "        x = countLogDf[column] > 0\n",
    "        v, m = labelStretches(x)\n",
    "        countLogDf[column] = v\n",
    "    return countLogDf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936a53aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def intrapolateUnderThreshold(s, th):\n",
    "    sOverTh = np.array([i for i, x in enumerate(s) if x != 'NaN' and x > th])\n",
    "    # print('removed indices ' + str(sOverTh))\n",
    "    s[sOverTh] = 'NaN'\n",
    "    s = np.array(s, dtype=np.float64)\n",
    "    nans, interpInd = np.isnan(s), lambda z: z.nonzero()[0]\n",
    "    s[nans] = np.interp(interpInd(nans), interpInd(~nans), s[~nans])\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edfca99",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def assignStatus(metaDataDf):\n",
    "    if 'Status' not in metaDataDf.columns:\n",
    "        metaDataDf.insert(1, 'Status', metaDataDf.Genotype, True)\n",
    "        metaDataDfCopy = metaDataDf.copy()\n",
    "        TestInd = [i for i, s in enumerate(\n",
    "            metaDataDf.Genotype) if 'w1118' not in s]\n",
    "        metaDataDfCopy['Status'] = 'Ctrl'\n",
    "        metaDataDfCopy.loc[TestInd, 'Status'] = 'Test'\n",
    "        metaDataDf = metaDataDfCopy\n",
    "    return metaDataDf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cca8083",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def fallEvents(countLogDf, nstd=4, windowsize=1000, ewm1=12, ewm2=26, ewm3=9):\n",
    "    # added Jan 2022 to detect falls\n",
    "    yy = countLogDf.filter(regex='_Y')\n",
    "    vx = countLogDf.filter(regex='_vX')\n",
    "    vy = countLogDf.filter(regex='_vY')\n",
    "    omega = pd.DataFrame(data=np.arctan(vx.values/vy.values), index=vy.index,\n",
    "                         columns=[c.split('_v')[0] + '_AV' for c in vy.columns])\n",
    "\n",
    "    exp1 = vy.ewm(span=ewm1, adjust=False).mean()\n",
    "    exp2 = vy.ewm(span=ewm2, adjust=False).mean()\n",
    "    macd = exp1-exp2\n",
    "    exp3 = macd.ewm(span=ewm3, adjust=False).mean()\n",
    "    a = np.zeros(vy.shape)\n",
    "    aa = np.zeros(vy.shape)\n",
    "    b = []\n",
    "    locoUtilities.startProgressbar()\n",
    "    for i in range(0, macd.shape[1]):\n",
    "        print(i)\n",
    "\n",
    "        a[:, i], b = labelStretches(macd.iloc[:, i]-exp3.iloc[:, i] < -0.025)\n",
    "        n = 1\n",
    "        for j in range(0, len(b)):\n",
    "            segstart = b.currIdx[j]\n",
    "            segend = b.currIdx[j]+b.inc[j]\n",
    "            speedthreshold = - np.nanmean(np.abs(vy.iloc[segstart-windowsize:segend+windowsize, i]))-np.nanstd(\n",
    "                np.abs(vy.iloc[segstart-windowsize:segend+windowsize, i]))*nstd\n",
    "            if len(vy.iloc[segstart: segend+1, i]) > 0:\n",
    "                if np.nanmin(vy.iloc[segstart: segend+1, i].values) < speedthreshold and np.nanmax(yy.iloc[segstart: segend+1, i].values) - np.nanmin(yy.iloc[segstart: segend, i].values) > 0.5:\n",
    "                    aa[segstart:segend, i] = n\n",
    "                    n = n + 1        \n",
    "        locoUtilities.drawProgressbar()\n",
    "    locoUtilities.endProgressbar()\n",
    "\n",
    "    falls = pd.DataFrame(data=aa, index=vy.index, columns=[\n",
    "                         c.split('_v')[0] + '_Falls' for c in vy.columns])\n",
    "    countLogDf.drop(list(countLogDf.filter(regex='_AV')), axis=1, inplace=True)\n",
    "    countLogDf.drop(list(countLogDf.filter(regex='_Falls')),\n",
    "                    axis=1, inplace=True)\n",
    "\n",
    "    newCountLog = pd.concat(\n",
    "        [countLogDf.iloc[:, [0, 1, 2]], omega, falls], axis=1)\n",
    "    newCountLog = pd.concat([countLogDf, omega, falls], axis=1)\n",
    "    return falls, newCountLog, macd\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
